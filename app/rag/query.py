"""Query engine for RAG-based question answering.

This module orchestrates the RAG pipeline:
1. Retrieve relevant documents (Phase 2+)
2. Generate answer using LLM with retrieved context
3. Return structured response with citations

Uses the `instructor` library for structured, Pydantic-validated LLM outputs.
"""

from functools import lru_cache

import instructor
from openai import OpenAI

from app.core.config import settings
from app.rag.models import Citation, LLMResponse, QueryResponse, RiskLevel
from app.rag.retriever import build_source_mapping, format_contexts_for_llm, retrieve


# =============================================================================
# LLM CLIENT (SINGLETON)
# =============================================================================
# Create the client once and reuse it across requests.
# This avoids the overhead of creating a new client per request.


@lru_cache(maxsize=1)
def get_llm_client() -> instructor.Instructor:
    """
    Get a cached instructor-patched OpenAI client.

    The client is created once and reused for all subsequent calls.
    This is more efficient than creating a new client per request.

    Returns:
        instructor.Instructor: OpenAI client with instructor patching
    """
    return instructor.from_openai(OpenAI(api_key=settings.openai_api_key))


# =============================================================================
# SYSTEM PROMPT
# =============================================================================


SYSTEM_PROMPT = """You are a home maintenance assistant. Answer questions about home maintenance, troubleshooting, and repairs.

You have access to a knowledge base of manuals, guides, and documentation about the user's home systems. When answering questions, you will receive relevant excerpts from these documents.

IMPORTANT RULES:
1. Assess risk level for every question:
   - LOW: Safe for any homeowner to do themselves
   - MED: Requires some caution or basic skills
   - HIGH: Involves gas, electrical, structural, or safety-critical work

2. If risk is HIGH, you MUST recommend calling a licensed professional (electrician, plumber, HVAC tech, etc.)

3. Be concise and actionable - homeowners want clear steps, not essays

4. ALWAYS cite your sources using the [Source N] format that matches the context provided. Include citations inline where relevant.

5. If the provided context does not contain enough information to answer the question reliably, say "I don't have enough information in my knowledge base to answer this question reliably." Do NOT make up information or cite sources that weren't provided.

6. Only cite information that actually appears in the provided context. Never hallucinate citations.
"""


# =============================================================================
# CITATION ENRICHMENT
# =============================================================================


def enrich_citations(
    llm_citations: list[Citation],
    source_mapping: dict[int, dict],
) -> list[Citation]:
    """
    Validate and enrich LLM-generated citations using retrieved node metadata.

    This function:
    1. Matches each LLM citation to a retrieved source
    2. Enriches with metadata from the source mapping
    3. Filters out citations that don't match any retrieved source

    Args:
        llm_citations: Citations generated by the LLM
        source_mapping: Mapping from source index to node metadata
            (from build_source_mapping())

    Returns:
        List of validated and enriched Citation objects.
        Citations that can't be matched to a source are filtered out.
    """
    if not source_mapping:
        return []

    enriched = []
    for citation in llm_citations:
        matched_source = _match_citation_to_source(citation, source_mapping)
        if matched_source:
            enriched_citation = _build_enriched_citation(citation, matched_source)
            enriched.append(enriched_citation)

    return enriched


def _match_citation_to_source(
    citation: Citation,
    source_mapping: dict[int, dict],
) -> dict | None:
    """
    Try to match an LLM citation to a retrieved source.

    Matching strategies (in order):
    1. Parse "Source N" pattern and look up by index
    2. Match against "file_name - device_name" format
    3. Match against just the file_name

    Args:
        citation: An LLM-generated citation
        source_mapping: Mapping from source index to metadata

    Returns:
        The matched source metadata dict, or None if no match found.
    """
    source_str = citation.source.strip()

    # Strategy 1: Parse "Source N" or "[Source N]" pattern
    import re
    match = re.search(r"[Ss]ource\s*(\d+)", source_str)
    if match:
        source_idx = int(match.group(1))
        if source_idx in source_mapping:
            return source_mapping[source_idx]

    # Strategy 2 & 3: Match against metadata strings
    for _idx, metadata in source_mapping.items():
        file_name = metadata.get("file_name", "")
        device_name = metadata.get("device_name", "")

        # Match against "file_name - device_name" format
        full_source = f"{file_name} - {device_name}"
        if full_source in source_str or source_str in full_source:
            return metadata

        # Match against just the file_name
        if file_name and file_name in source_str:
            return metadata

    return None


def _build_enriched_citation(citation: Citation, source_metadata: dict) -> Citation:
    """
    Build an enriched citation using source metadata.

    Preserves LLM-provided fields (page, section, quote) when present,
    but ensures the source field uses the canonical file name.

    Args:
        citation: Original LLM citation
        source_metadata: Metadata from the matched retrieved source

    Returns:
        New Citation with enriched/validated fields
    """
    # Build a descriptive source string
    file_name = source_metadata.get("file_name", "Unknown")
    device_name = source_metadata.get("device_name", "")

    if device_name:
        source = f"{file_name} - {device_name}"
    else:
        source = file_name

    return Citation(
        source=source,
        page=citation.page,  # Preserve LLM's page reference
        section=citation.section,  # Preserve LLM's section reference
        quote=citation.quote,  # Preserve LLM's quote
    )


# =============================================================================
# QUERY FUNCTION
# =============================================================================


def _has_sufficient_evidence(nodes: list) -> bool:
    """
    Check if retrieved nodes have sufficient relevance to answer the question.

    Args:
        nodes: Retrieved nodes with scores

    Returns:
        True if there's sufficient evidence, False otherwise.
    """
    if not nodes:
        return False

    # Check if the best match meets the minimum relevance threshold
    top_score = nodes[0].score
    return top_score >= settings.rag.min_relevance_score


def query(question: str) -> QueryResponse:
    """
    Query the system with a question and get a structured response.

    This is the main entry point for the RAG pipeline. It:
    1. Retrieves relevant documents from the vector index
    2. Checks if retrieval found sufficiently relevant content
    3. Calls the LLM with retrieved context (or returns fallback)
    4. Returns a structured response with citations

    Args:
        question: The user's question about home maintenance.

    Returns:
        QueryResponse with answer, citations, risk level, and contexts.
    """
    # Retrieve relevant chunks from the knowledge base
    retrieved_nodes = retrieve(question)

    # Check if we have sufficient evidence to answer
    if not _has_sufficient_evidence(retrieved_nodes):
        return QueryResponse(
            answer=(
                "I don't have enough information in my knowledge base to answer "
                "this question reliably. Please try rephrasing your question or "
                "ask about a topic covered in the available documentation."
            ),
            citations=[],
            risk_level=RiskLevel.LOW,
            contexts=[],
        )

    # Extract contexts for response
    context = format_contexts_for_llm(retrieved_nodes)
    contexts = [node.node.text for node in retrieved_nodes]

    # Build source mapping for citation validation
    source_mapping = build_source_mapping(retrieved_nodes)

    # Get cached LLM client
    client = get_llm_client()

    # Build user message with retrieved context
    user_message = f"""Context from your knowledge base:
{context}

Question: {question}

Answer based on the context above. Cite sources using [Source N] format."""

    # Call LLM with structured output
    # instructor automatically:
    # 1. Converts LLMResponse to a JSON schema
    # 2. Uses OpenAI function calling to enforce the schema
    # 3. Validates the response against the Pydantic model
    # 4. Retries if validation fails (configurable)
    llm_response = client.chat.completions.create(
        model=settings.llm.model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_message},
        ],
        response_model=LLMResponse,
        temperature=settings.llm.temperature,
        max_completion_tokens=settings.llm.max_completion_tokens,
    )

    # Validate and enrich citations using retrieved source metadata
    citations = enrich_citations(llm_response.citations, source_mapping)

    return QueryResponse(
        answer=llm_response.answer,
        citations=citations,
        risk_level=RiskLevel(llm_response.risk_level),
        contexts=contexts,
    )
